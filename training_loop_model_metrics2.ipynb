{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import csv\n",
    "import base64\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import whisper\n",
    "import neologdn\n",
    "import evaluate\n",
    "import MeCab\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import WhisperTokenizerFast\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import torchaudio.transforms as at\n",
    "import transformers\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class GroupNorm(nn.Module): #GroupNormRMSNorm\n",
    "    def __init__(self, num_groups, num_channels, eps=1e-6):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(num_channels))\n",
    "        self.b = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.scale = nn.Parameter(torch.ones(num_channels))\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups, num_channels, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.group_norm(x)\n",
    "        norm_x = torch.norm(x, dim=-1, keepdim=True)\n",
    "        rms_x = x / norm_x\n",
    "        scaled_x = rms_x * self.scale.view(1, -1, 1) \n",
    "        x = self.g.view(1, -1, 1) * scaled_x + self.b.view(1, -1, 1)\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        return x\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight) \n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _conv_forward(self, x: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return super()._conv_forward(x, weight, bias)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def rotate_queries_or_keys(self, x):\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', torch.arange(x.shape[1], device=x.device), self.inv_freq) \n",
    "        sin = sinusoid_inp.sin()[None, :, None, :] \n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x\n",
    "\n",
    "class SinusoidalFeatures:\n",
    "    def __init__(self, n_ctx, n_state):\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.features = self.sinusoidal_features(n_ctx, n_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_features(n_ctx, n_state):\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        return features\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.features\n",
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module): \n",
    "    def __init__(self, n_ctx, n_state, gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "        sinusoidal_embeddings = SinusoidalFeatures(n_ctx, n_state)()\n",
    "        self.positional_embeddings = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.gradient_checkpointing:\n",
    "            position_embeddings = checkpoint.checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = F.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n",
    "\n",
    "class BiasedCrossAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(n_head, self.head_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None):\n",
    "        q = self.query(q).view(q.shape[0], q.shape[1], self.n_head, self.head_dim)\n",
    "        k = self.key(k).view(k.shape[0], k.shape[1], self.n_head, self.head_dim)\n",
    "        v = self.value(v).view(v.shape[0], v.shape[1], self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / self.head_dim ** 0.5 + self.bias\n",
    "        if mask is not None:\n",
    "            qk += mask\n",
    "\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = (w @ v).permute(0, 2, 1, 3).reshape(q.shape[0], q.shape[1], -1)\n",
    "        out = self.norm(self.out(out) + q.view(q.shape[0], q.shape[1], -1))\n",
    "        return out\n",
    "\n",
    "class AugmentedMemory(nn.Module):\n",
    "    def __init__(self, n_state: int, memory_size: int, n_head: int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.zeros(memory_size, n_state))\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.memory_update = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        bsz, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.query(x).view(bsz, seq_len, self.n_head, self.head_dim)\n",
    "        k = self.key(self.memory).view(self.memory.size(0), self.n_head, self.head_dim)\n",
    "        v = self.value(self.memory).view(self.memory.size(0), self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / self.head_dim ** 0.5\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        memory_out = (w @ v).permute(0, 2, 1, 3).reshape(bsz, seq_len, -1)\n",
    "        memory_out = self.norm(self.out(memory_out) + x)\n",
    "\n",
    "        new_memory = self.memory_update(memory_out)\n",
    "        self.memory = self.memory + new_memory.mean(dim=0)\n",
    "        return memory_out\n",
    "\n",
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, kernel_size=3, dropout_rate=0.1, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size//2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        if use_GroupNorm:\n",
    "            self.norm = nn.GroupNorm(num_groups=n_head, num_channels=n_state)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(n_state)\n",
    "        \n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        x = x.permute(0, 2, 1) \n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1) \n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        return self.out(conv_out) + x.permute(0, 2, 1) \n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, window_size=1.0, dropout_rate=0.1, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.global_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        \n",
    "        self.use_GroupNorm = use_GroupNorm\n",
    "        if self.use_GroupNorm:\n",
    "            self.ln_local = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "            self.ln_global = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_local = LayerNorm(n_state)\n",
    "            self.ln_global = LayerNorm(n_state)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.window_size = window_size \n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x_local = self.ln_local(x)\n",
    "        x_global = self.ln_global(x)\n",
    "\n",
    "        x_local = x_local.permute(1, 0, 2)\n",
    "        x_global = x_global.permute(1, 0, 2)\n",
    "\n",
    "        local_out = self.sliding_window_attention(x_local)\n",
    "        global_out, _ = self.global_attn(x_global, x_global, x_global)\n",
    "\n",
    "        combined_out = local_out + global_out\n",
    "        combined_out = combined_out.permute(1, 0, 2)\n",
    "\n",
    "        return self.dropout(combined_out)\n",
    "\n",
    "    def sliding_window_attention(self, x):\n",
    "        seq_len, batch_size, n_state = x.size()\n",
    "        window_size = int(self.window_size)\n",
    "\n",
    "        output = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(seq_len, i + window_size + 1)\n",
    "            query = x[i:i+1, :, :]\n",
    "            key = x[start:end, :, :]\n",
    "            value = x[start:end, :, :]\n",
    "\n",
    "            attn_output, _ = self.local_attn(query, key, value)\n",
    "            output[i:i+1, :, :] = attn_output\n",
    "\n",
    "        return output\n",
    "    \n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, use_dynamic_conv: bool = False, use_hybrid_attention: bool = False,\n",
    "                 use_biased_attention: bool = False, use_augmented_memory: bool = False,\n",
    "                 cross_attention: bool = False, dropout_rate=0.1, gradient_checkpointing=False, use_GroupNorm=False, window_size: int = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_dynamic_conv = use_dynamic_conv\n",
    "        self.use_hybrid_attention = use_hybrid_attention\n",
    "        self.use_biased_attention = use_biased_attention\n",
    "        self.use_augmented_memory = use_augmented_memory\n",
    "\n",
    "        if self.use_dynamic_conv:\n",
    "            self.attn = DynamicConvAttention(n_state, n_head, dropout_rate=dropout_rate)\n",
    "        elif self.use_hybrid_attention:\n",
    "            self.attn = HybridAttention(n_state, n_head, window_size=window_size, dropout_rate=dropout_rate, use_GroupNorm=use_GroupNorm)\n",
    "        elif self.use_biased_attention:\n",
    "            self.attn = BiasedCrossAttention(n_state, n_head, dropout_rate=dropout_rate)\n",
    "        elif self.use_augmented_memory:\n",
    "            self.attn = AugmentedMemory(n_state, memory_size=512, n_head=n_head, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            self.attn = MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, use_GroupNorm=use_GroupNorm)\n",
    "\n",
    "        self.attn_ln = GroupNorm(num_groups=4, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "        self.cross_attention = cross_attention\n",
    "        if self.cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, use_GroupNorm=use_GroupNorm)\n",
    "            self.cross_attn_ln = GroupNorm(num_groups=4, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp),\n",
    "            GroupNorm(num_groups=4, num_channels=n_mlp) if use_GroupNorm else LayerNorm(n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = GroupNorm(num_groups=4, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None,\n",
    "                mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "\n",
    "        attn_input = self.attn_ln(x)\n",
    "\n",
    "        if self.gradient_checkpointing:\n",
    "            if any([self.use_hybrid_attention, self.use_dynamic_conv, self.use_biased_attention, self.use_augmented_memory]):\n",
    "                attn_out = x + checkpoint.checkpoint(self.attn, attn_input)\n",
    "            else:\n",
    "                attn_out = x + checkpoint.checkpoint(self.attn, attn_input, mask, kv_cache)[0]\n",
    "        else:\n",
    "            if any([self.use_hybrid_attention, self.use_dynamic_conv, self.use_biased_attention, self.use_augmented_memory]):\n",
    "                attn_out = x + self.attn(attn_input)\n",
    "            else:\n",
    "                attn_out = x + self.attn(attn_input, mask=mask, kv_cache=kv_cache)[0]\n",
    "\n",
    "        if self.cross_attention and xa is not None:\n",
    "            cross_attn_input = self.cross_attn_ln(attn_out)\n",
    "\n",
    "            if self.gradient_checkpointing:\n",
    "                attn_out = attn_out + checkpoint.checkpoint(self.cross_attn, cross_attn_input, xa, kv_cache)[0]\n",
    "            else:\n",
    "                attn_out = attn_out + self.cross_attn(cross_attn_input, xa, kv_cache=kv_cache)[0]\n",
    "\n",
    "        mlp_input = self.mlp_ln(attn_out)\n",
    "\n",
    "        if self.gradient_checkpointing:\n",
    "            mlp_out = attn_out + checkpoint.checkpoint(self.mlp, mlp_input)\n",
    "        else:\n",
    "            mlp_out = attn_out + self.mlp(mlp_input)\n",
    "\n",
    "        return mlp_out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int, dropout_rate=0.1, gradient_checkpointing=False, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim) \n",
    "        self.temperature = nn.Parameter(torch.ones(1) * (self.head_dim ** -0.5)) \n",
    "        self.dropout = nn.Dropout(dropout_rate) \n",
    "\n",
    "        self.use_GroupNorm = use_GroupNorm\n",
    "        self.attn_ln = GroupNorm(num_groups=1, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "        \n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        x_norm = self.attn_ln(x)\n",
    "\n",
    "        q = self.query(x_norm)\n",
    "        k = self.key(x_norm if xa is None else xa)\n",
    "        v = self.value(x_norm if xa is None else xa)\n",
    "\n",
    "        if kv_cache is not None and self.key in kv_cache:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_head, -1)\n",
    "\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv) + x, qk \n",
    "\n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = self.temperature\n",
    "\n",
    "        q = q.view(n_batch, n_ctx, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(n_batch, k.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(n_batch, v.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k.transpose(-2, -1) * scale)\n",
    "            if mask is not None:\n",
    "                qk += mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            w = self.dropout(w)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = qk.detach()\n",
    "        \n",
    "        return out, qk\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    main_input_name = \"input_features\"\n",
    "\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, use_dynamic_conv: bool = False, use_hybrid_attention: bool = False, use_biased_attention: bool = False, use_augmented_memory: bool = False, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.use_GroupNorm = use_GroupNorm\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                use_dynamic_conv=use_dynamic_conv,\n",
    "                use_hybrid_attention=use_hybrid_attention,\n",
    "                use_biased_attention=use_biased_attention,\n",
    "                use_augmented_memory=use_augmented_memory,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                use_GroupNorm=use_GroupNorm\n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        if self.use_GroupNorm:\n",
    "            self.ln_post = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, use_dynamic_conv: bool = False, use_hybrid_attention: bool = False, use_biased_attention: bool = False, use_augmented_memory: bool = False, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, gradient_checkpointing=gradient_checkpointing)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.use_GroupNorm = use_GroupNorm\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                use_dynamic_conv=use_dynamic_conv,\n",
    "                use_hybrid_attention=use_hybrid_attention,\n",
    "                use_biased_attention=use_biased_attention,\n",
    "                use_augmented_memory=use_augmented_memory,\n",
    "                cross_attention=True,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                use_GroupNorm=use_GroupNorm\n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        if self.use_GroupNorm:\n",
    "            self.ln_post = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "        positions = torch.arange(x.shape[1], device=x.device)\n",
    "        pos_emb = self.positional_embedding(positions).unsqueeze(0) \n",
    "        x = self.token_embedding(x) + pos_emb\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x, xa, self.mask, kv_cache)\n",
    "            else:\n",
    "                x = block(x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        logits = (x @ self.token_embedding.weight.to(x.dtype).T).float()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions, n_vocab, dropout_rate=0.1, gradient_checkpointing=False,\n",
    "                 use_dynamic_conv=False, use_hybrid_attention=False, use_biased_attention=False, use_augmented_memory=False, use_GroupNorm=False):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.n_vocab = n_vocab\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "            dropout_rate=dropout_rate,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            use_dynamic_conv=use_dynamic_conv,\n",
    "            use_hybrid_attention=use_hybrid_attention,\n",
    "            use_biased_attention=use_biased_attention,\n",
    "            use_augmented_memory=use_augmented_memory,\n",
    "            use_GroupNorm=use_GroupNorm\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer,\n",
    "            dropout_rate=dropout_rate,\n",
    "            #gradient_checkpointing=gradient_checkpointing, # needs debugging\n",
    "            use_dynamic_conv=use_dynamic_conv,\n",
    "            use_hybrid_attention=use_hybrid_attention,\n",
    "            use_biased_attention=use_biased_attention,\n",
    "            use_augmented_memory=use_augmented_memory,\n",
    "            use_GroupNorm=use_GroupNorm\n",
    "        )\n",
    "        all_heads = torch.zeros(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.dims.n_text_layer // 2:] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def forward(self, audio_features: torch.Tensor, input_ids: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        audio_features_encoded = self.encoder(audio_features)\n",
    "        logits = self.decoder(input_ids, audio_features_encoded)\n",
    "        \n",
    "        loss = None\n",
    "        if input_ids is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.n_vocab)\n",
    "            input_ids = input_ids.view(-1).long()\n",
    "            loss = loss_fct(logits, input_ids)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits, \"audio_features_encoded\": audio_features_encoded}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.dims.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head\n",
    "        )\n",
    "        self.register_buffer('alignment_heads', mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, audio_features: torch.Tensor):\n",
    "        return self.encoder(audio_features)\n",
    "\n",
    "    def logits(self, input_ids: torch.Tensor, audio_features: torch.Tensor):\n",
    "        return self.decoder(input_ids, audio_features)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_path, sample_rate: int = 16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, tokenizer, sample_rate=16000):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) \n",
    "            for row in reader:\n",
    "                audio_path, label = row[0], row[1]\n",
    "                self.samples.append((audio_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.samples[idx]\n",
    "        audio = f'{self.audio_dir}/{audio_path}'\n",
    "        return {\n",
    "            'audio_features': audio,\n",
    "            'labels': label,\n",
    "            'input_ids': label \n",
    "        }\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer, sample_rate=16000, n_mels=80):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "    def __call__(self, features):\n",
    "        audio_features, dec_input_ids, labels = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            audio_path = f['audio_features']\n",
    "            audio, _ = torchaudio.load(audio_path, normalize=True)\n",
    "            audio = whisper.pad_or_trim(audio.flatten())\n",
    "            audio = whisper.log_mel_spectrogram(audio, n_mels=self.n_mels)\n",
    "            \n",
    "            encoded_input = self.tokenizer.encode(f['labels'])\n",
    "            encoded_label = self.tokenizer.encode(f['labels'])\n",
    "\n",
    "            dec_input_ids.append([self.tokenizer.bos_token_id] + encoded_input)\n",
    "            labels.append(encoded_label + [self.tokenizer.eos_token_id])\n",
    "            audio_features.append(audio)\n",
    "\n",
    "        audio_features = torch.stack(audio_features)\n",
    "\n",
    "        input_lengths = [len(ids) for ids in dec_input_ids]\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_len = max(input_lengths + label_lengths)\n",
    "\n",
    "        dec_input_ids = [np.pad(ids, (0, max_len - len(ids)), 'constant', constant_values=self.tokenizer.pad_token_id) for ids in dec_input_ids]\n",
    "        labels = [np.pad(lab, (0, max_len - len(lab)), 'constant', constant_values=-100) for lab in labels]\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": dec_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"audio_features\": audio_features\n",
    "        }\n",
    "        batch = {k: torch.tensor(v, requires_grad=False) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred[\"predictions\"]\n",
    "    label_ids = pred[\"label_ids\"]\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = 100 * metrics_cer.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "checkpoint_dir = 'D:/proj3/ckpt/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "log_dir = 'D:/proj3/ckpt/logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(log_dir, 'training.log'), \n",
    "    filemode='w', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, eval_loader, optimizer, loss_fn, scheduler, num_epochs=1, max_steps=None, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=10, eval_interval=20, save_interval=100, checkpoint_dir=\"checkpoint_dir\", log_dir=\"log_dir\"):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if max_steps is not None and global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            if max_steps is not None and global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                audio_features = batch['audio_features'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].long().to(device)\n",
    "            except KeyError as e:\n",
    "                print(f\"Key error: {e}. Available keys in batch: {batch.keys()}\")\n",
    "                continue\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                with record_function(\"model_training\"):\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        audio_features_encoded = model.encoder(audio_features)\n",
    "                        decoder_output = model.decoder(input_ids, audio_features_encoded)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        total_loss += loss.item()\n",
    "                        loss = loss / accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if clear_cache:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "            global_step += 1\n",
    "            end_time = time.time()\n",
    "            samples_per_sec = len(batch['audio_features']) / (end_time - start_time)\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', total_loss / (step + 1), global_step)\n",
    "                writer.add_scalar('GradientNorm', total_norm, global_step)\n",
    "                writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                writer.add_scalar('SamplesPerSec', samples_per_sec, global_step)\n",
    "                writer.add_scalar(\"Memory/Allocated\", torch.cuda.memory_allocated(), global_step)\n",
    "                writer.add_scalar(\"Memory/Cached\", torch.cuda.memory_reserved(), global_step)\n",
    "                \n",
    "            if global_step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                all_predictions = []\n",
    "                all_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for eval_batch in eval_loader:\n",
    "                        try:\n",
    "                            audio_features = eval_batch['audio_features'].to(device)\n",
    "                            input_ids = eval_batch['input_ids'].to(device)\n",
    "                            labels = eval_batch['labels'].long().to(device)\n",
    "                        except KeyError as e:\n",
    "                            print(f\"Key error: {e}. Available keys in eval batch: {eval_batch.keys()}\")\n",
    "                            continue\n",
    "\n",
    "                        audio_features_encoded = model.encoder(audio_features)\n",
    "                        decoder_output = model.decoder(input_ids, audio_features_encoded)\n",
    "\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        eval_loss += loss.item()\n",
    "\n",
    "                        all_predictions.extend(torch.argmax(decoder_output, dim=-1).cpu().numpy().tolist())\n",
    "                        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "                predictions = {\n",
    "                    \"predictions\": np.array(all_predictions, dtype=\"object\"),\n",
    "                    \"label_ids\": np.array(all_labels, dtype=\"object\")\n",
    "                }\n",
    "\n",
    "                metrics = compute_metrics(predictions)\n",
    "                writer.add_scalar('Loss/eval', eval_loss / len(eval_loader), global_step)\n",
    "                writer.add_scalar('CER', metrics['cer'], global_step)\n",
    "                # writer.add_histogram('Predictions', torch.argmax(logits, dim=-1), global_step)\n",
    "                # writer.add_histogram('Labels', labels, global_step)\n",
    "\n",
    "                scheduler.step(eval_loss / len(eval_loader))\n",
    "\n",
    "                sample_indices = range(min(1, len(all_predictions))) \n",
    "                for idx in sample_indices:\n",
    "                    pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                    label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                    print(f\"Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "                    logging.info(f\"Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if global_step % save_interval == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "                logging.info(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "        logging.info(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    final_model_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    logging.info(f\"Final model saved to {final_model_path}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, device, tokenizer):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                audio_features = batch['audio_features'].to(device)\n",
    "                labels = batch['labels'].long().to(device)\n",
    "            except KeyError as e:\n",
    "                print(f\"Key error: {e}. Available keys in batch: {batch.keys()}\")\n",
    "                continue\n",
    "\n",
    "            outputs = model(audio_features, labels=labels)\n",
    "            logits = outputs['logits']\n",
    "            loss = outputs['loss'] if 'loss' in outputs else loss_fct(logits.view(-1, model.n_vocab), labels.view(-1))\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            all_predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    eval_loss /= len(eval_loader)\n",
    "    metrics = compute_metrics({\"predictions\": np.array(all_predictions, dtype=\"object\"), \"label_ids\": np.array(all_labels, dtype=\"object\")})\n",
    "    return eval_loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_mels\": 80,\n",
    "    \"n_audio_ctx\": 1500,\n",
    "    \"n_audio_state\": 1024,\n",
    "    \"n_audio_head\": 14,\n",
    "    \"n_audio_layer\": 8,\n",
    "    \"n_vocab\": 51865,\n",
    "    \"n_text_ctx\": 448,\n",
    "    \"n_text_state\": 1024,\n",
    "    \"n_text_head\": 14,\n",
    "    \"n_text_layer\": 8\n",
    "}\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "dimensions = ModelDimensions(\n",
    "    n_mels=80, \n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_state=768, \n",
    "    n_audio_head=12, \n",
    "    n_audio_layer=12, \n",
    "    n_vocab=51865, \n",
    "    n_text_ctx=448,\n",
    "    n_text_state=768, \n",
    "    n_text_head=12, \n",
    "    n_text_layer=12\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    tokenizer = WhisperTokenizerFast.from_pretrained(\"D:/good/my_tokenizer\", task=\"transcribe\", language=\"japanese\", local_files_only=True)\n",
    "    csv_file = 'D:/proj/datasets/gvj/trimmed/metadata.csv'\n",
    "    audio_dir = 'D:/proj/datasets/gvj/trimmed/'\n",
    "\n",
    "    metrics_cer = evaluate.load(\"cer\")\n",
    "\n",
    "    def train_val_dataset(dataset, val_split=0.001):\n",
    "        train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "        datasets = {}\n",
    "        datasets['train'] = Subset(dataset, train_idx)\n",
    "        datasets['val'] = Subset(dataset, val_idx)\n",
    "        return datasets\n",
    "\n",
    "    dataset = CustomAudioDataset(csv_file, audio_dir, tokenizer)\n",
    "    datasets = train_val_dataset(dataset)\n",
    "    train_dataset = datasets['train']\n",
    "    eval_dataset = datasets['val']\n",
    "\n",
    "    def train_dataloader():   \n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1, \n",
    "            drop_last=True, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def eval_dataloader():\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=1, \n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "    collate_fn = WhisperDataCollatorWithPadding(tokenizer)\n",
    "    train_loader = train_dataloader()\n",
    "    eval_loader = eval_dataloader()\n",
    "\n",
    "    model = Whisper(dimensions, n_vocab=51865, dropout_rate=0.0, \n",
    "                    gradient_checkpointing=True, \n",
    "                    use_hybrid_attention=False, \n",
    "                    use_GroupNorm=True,\n",
    "                    use_dynamic_conv=True,\n",
    "                    use_biased_attention=False,\n",
    "                    use_augmented_memory=False,\n",
    "                    ).cuda()\n",
    "\n",
    "    optimizer = optim.Adafactor(\n",
    "        model.parameters(), \n",
    "        lr=0.025, \n",
    "        beta2_decay=-0.8, \n",
    "        eps=(None, 0.001), \n",
    "        d=1.0, \n",
    "        weight_decay=0.0, \n",
    "        foreach=None, \n",
    "        maximize=False\n",
    "    )\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        eps=1e-08\n",
    "    )\n",
    "\n",
    "    train_and_evaluate(model, train_loader, eval_loader, optimizer, loss_fn, scheduler, num_epochs=1, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=5, eval_interval=10, save_interval=1000, checkpoint_dir=checkpoint_dir, log_dir=log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, kernel_size=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size//2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Determine dynamic dimensions\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, embed_dim, seq_len)\n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (batch_size, seq_len, embed_dim)\n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        return self.out(conv_out) + x.permute(0, 2, 1)  # Ensure x is permuted back to original shape\n",
    "    \n",
    "# Example input tensor\n",
    "x = torch.randn(32, 512, 768)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Initialize DynamicConvAttention\n",
    "dynamic_conv_attn = DynamicConvAttention(n_state=768, n_head=8, kernel_size=3, dropout_rate=0.1)\n",
    "\n",
    "# Forward pass\n",
    "output = dynamic_conv_attn(x)\n",
    "print(\"Output shape:\", output.shape)  # Expected output shape: (batch_size, seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Helper function to transfer layers\n",
    "# def transfer_layer(source_model, target_model, source_layer_name, target_layer_name):\n",
    "#     source_layer = dict(source_model.named_parameters()).get(source_layer_name)\n",
    "#     target_layer = dict(target_model.named_parameters()).get(target_layer_name)\n",
    "#     if source_layer is not None and target_layer is not None:\n",
    "#         target_layer.data.copy_(source_layer.data)\n",
    "\n",
    "# # Load pretrained model\n",
    "# pretrained_model_name = \"openai/whisper-small\"\n",
    "# pretrained_model = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name)\n",
    "# model_state_dict = model.state_dict()\n",
    "\n",
    "# transfer_layer(pretrained_model, model, 'model.decoder.embed_tokens.weight', 'decoder.token_embedding.weight')\n",
    "\n",
    "# # Encoder layers\n",
    "# for i in range(len(model.encoder.blocks)):\n",
    "#     # Attention layers\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.self_attn.q_proj.weight', f'encoder.blocks.{i}.attn.query.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.self_attn.k_proj.weight', f'encoder.blocks.{i}.attn.key.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.self_attn.v_proj.weight', f'encoder.blocks.{i}.attn.value.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.self_attn.out_proj.weight', f'encoder.blocks.{i}.attn.out.weight')\n",
    "    \n",
    "#     # MLP layers\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.fc1.weight', f'encoder.blocks.{i}.mlp.0.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.fc1.bias', f'encoder.blocks.{i}.mlp.0.bias')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.fc2.weight', f'encoder.blocks.{i}.mlp.2.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.encoder.layers.{i}.fc2.bias', f'encoder.blocks.{i}.mlp.2.bias')\n",
    "\n",
    "# # Decoder layers\n",
    "# for i in range(len(model.decoder.blocks)):\n",
    "#     # Self attention\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.self_attn.q_proj.weight', f'decoder.blocks.{i}.attn.query.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.self_attn.k_proj.weight', f'decoder.blocks.{i}.attn.key.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.self_attn.v_proj.weight', f'decoder.blocks.{i}.attn.value.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.self_attn.out_proj.weight', f'decoder.blocks.{i}.attn.out.weight')\n",
    "    \n",
    "#     # Cross attention\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.cross_attn.q_proj.weight', f'decoder.blocks.{i}.cross_attn.query.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.cross_attn.k_proj.weight', f'decoder.blocks.{i}.cross_attn.key.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.cross_attn.v_proj.weight', f'decoder.blocks.{i}.cross_attn.value.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.cross_attn.out_proj.weight', f'decoder.blocks.{i}.cross_attn.out.weight')\n",
    "    \n",
    "#     # MLP layers\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.fc1.weight', f'decoder.blocks.{i}.mlp.0.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.fc1.bias', f'decoder.blocks.{i}.mlp.0.bias')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.fc2.weight', f'decoder.blocks.{i}.mlp.2.weight')\n",
    "#     transfer_layer(pretrained_model, model, f'model.decoder.layers.{i}.fc2.bias', f'decoder.blocks.{i}.mlp.2.bias')\n",
    "\n",
    "# print(\"Full layer transfer complete! Ready to test training.\")\n",
    "# model.load_state_dict(model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
